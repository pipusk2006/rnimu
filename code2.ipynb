{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be0aca27",
   "metadata": {},
   "source": [
    "1) Загрузка X/y + группы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314b68e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BUILD X/y FROM FILES (supports new \"forest_1.biom\" and old \"(2.5).biom\") ===\n",
    "from pathlib import Path\n",
    "import re, numpy as np, pandas as pd\n",
    "import h5py\n",
    "\n",
    "DATA_DIR = Path(\".\")         # корневая папка с .biom / .tsv\n",
    "ALLOW_TSV = False            # если начнёшь качать TSV, поставь True и реализуй read_tsv_genus()\n",
    "\n",
    "# === КАНОН классов (короткие → онтология MGnify) ===\n",
    "CLASS_CANON = {\n",
    "    \"forest\":       \"root:Environmental:Terrestrial:Soil:Forest soil\",\n",
    "    \"wetland\":      \"root:Environmental:Terrestrial:Soil:Wetlands\",\n",
    "    \"grassland\":    \"root:Environmental:Terrestrial:Soil:Grasslands\",\n",
    "    \"desert\":       \"root:Environmental:Terrestrial:Soil:Desert\",\n",
    "    \"agricultural\": \"root:Environmental:Terrestrial:Soil:Agricultural\",\n",
    "}\n",
    "ALLOWED = set(CLASS_CANON.keys())\n",
    "\n",
    "# Синонимы/наследие → к канону (и старые ярлыки)\n",
    "NAME_NORMALIZE = {\n",
    "    \"tropical_forest\": \"forest\",\n",
    "    \"peat_bog\": \"wetland\",\n",
    "    \"temperate_agri\": \"agricultural\",\n",
    "    \"grassland_cropland\": \"grassland\",\n",
    "    \"swamp\": \"wetland\",\n",
    "    \"meadow\": \"grassland\",\n",
    "    \"cropland\": \"agricultural\",\n",
    "    \"agri\": \"agricultural\",\n",
    "    \"farm\": \"agricultural\",\n",
    "}\n",
    "\n",
    "# Что делать с \"arctic\"/\"tundra\" из старых наборов: \"drop\" или \"map_to_grassland\"\n",
    "LEGACY_ARCTIC_POLICY = \"drop\"   # или \"map_to_grassland\"\n",
    "if LEGACY_ARCTIC_POLICY == \"map_to_grassland\":\n",
    "    NAME_NORMALIZE[\"arctic\"] = \"grassland\"\n",
    "    NAME_NORMALIZE[\"tundra\"] = \"grassland\"\n",
    "\n",
    "def normalize_label(raw: str):\n",
    "    \"\"\"raw -> (short_label, ontology_str) или (None, None) если отбрасываем.\"\"\"\n",
    "    if raw is None:\n",
    "        return None, None\n",
    "    s = raw.strip().lower()\n",
    "    s = NAME_NORMALIZE.get(s, s)\n",
    "    if s == \"arctic\" and LEGACY_ARCTIC_POLICY == \"drop\":\n",
    "        return None, None\n",
    "    if s in ALLOWED:\n",
    "        return s, CLASS_CANON[s]\n",
    "    return None, None\n",
    "\n",
    "# --- парсеры имён ---\n",
    "def parse_label_from_new(name: str):\n",
    "    # Новый формат: <label>_<tag>.<ext>, напр.: forest_1.biom, wetland-07.tsv\n",
    "    m = re.match(r\"^([A-Za-z][A-Za-z0-9\\-]*)[_\\-]([A-Za-z0-9]+)\\.(biom|tsv)$\", name)\n",
    "    if not m:\n",
    "        return None, None\n",
    "    raw = m.group(1)\n",
    "    lab, _ = normalize_label(raw)\n",
    "    tag = m.group(2)\n",
    "    return lab, tag\n",
    "\n",
    "def parse_label_from_old(name: str):\n",
    "    # Старый формат: ... (N[.tag]).biom → мапим цифру к канону\n",
    "    m = re.search(r\"\\(([\\d\\.]+)\\)\\.biom$\", name)\n",
    "    if not m:\n",
    "        return None, None\n",
    "    token = m.group(1)\n",
    "    biome_digit = token.split(\".\", 1)[0]\n",
    "    BIOME_MAP_OLD = {\n",
    "        \"1\": \"desert\",\n",
    "        \"2\": \"arctic\",\n",
    "        \"3\": \"forest\",\n",
    "        \"4\": \"wetland\",\n",
    "        \"6\": \"agricultural\",\n",
    "        # \"5\": volcanic — пропускаем\n",
    "    }\n",
    "    raw = BIOME_MAP_OLD.get(biome_digit)\n",
    "    lab, _ = normalize_label(raw)\n",
    "    tag = token.split(\".\", 1)[1] if \".\" in token else None\n",
    "    return lab, tag\n",
    "\n",
    "def clean_stem(stem: str) -> str:\n",
    "    s = stem.strip().replace(\"— копия\", \"\").replace(\"копия\", \"\")\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^A-Za-z0-9_.()-]+\", \"_\", s)\n",
    "    return s\n",
    "\n",
    "# --- чтение BIOM (HDF5) → Genus профиль ---\n",
    "def read_biom_hdf5_genus(path: Path) -> pd.Series:\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        obs_ids = [s.decode() if isinstance(s, (bytes, np.bytes_)) else str(s)\n",
    "                   for s in f[\"observation/ids\"][...]]\n",
    "        sample_ids = [s.decode() if isinstance(s, (bytes, np.bytes_)) else str(s)\n",
    "                      for s in f[\"sample/ids\"][...]]\n",
    "        n_obs, n_samples = len(obs_ids), len(sample_ids)\n",
    "        data    = f[\"observation/matrix/data\"][...]\n",
    "        indices = f[\"observation/matrix/indices\"][...]\n",
    "        indptr  = f[\"observation/matrix/indptr\"][...]\n",
    "        # taxonomy\n",
    "        tax = []\n",
    "        tax_ds = f.get(\"observation/metadata/taxonomy\")\n",
    "        if tax_ds is not None:\n",
    "            raw = tax_ds[...]\n",
    "            for item in raw:\n",
    "                if isinstance(item, (np.ndarray, list)):\n",
    "                    parts = [(e.decode() if isinstance(e, (bytes, np.bytes_)) else str(e)) for e in item]\n",
    "                    tax.append(\"; \".join(parts))\n",
    "                else:\n",
    "                    tax.append(item.decode() if isinstance(item, (bytes, np.bytes_)) else str(item))\n",
    "        else:\n",
    "            tax = [\"\"] * n_obs\n",
    "        def to_genus(t: str) -> str:\n",
    "            parts = [p.strip() for p in str(t).split(\";\")]\n",
    "            for p in parts:\n",
    "                if p.startswith(\"g__\"):\n",
    "                    v = p[3:].strip()\n",
    "                    if v:\n",
    "                        return v\n",
    "            for p in reversed(parts):\n",
    "                v = re.sub(r\"^[a-z]__\", \"\", p.strip())\n",
    "                if v:\n",
    "                    return v\n",
    "            return \"Unclassified\"\n",
    "        genus = [to_genus(t) for t in tax]\n",
    "        per_sample = [dict() for _ in range(n_samples)]\n",
    "        for i in range(n_obs):\n",
    "            a, b = indptr[i], indptr[i+1]\n",
    "            if a == b: \n",
    "                continue\n",
    "            g = genus[i]\n",
    "            cols = indices[a:b]\n",
    "            vals = data[a:b]\n",
    "            for c, v in zip(cols, vals):\n",
    "                d = per_sample[int(c)]\n",
    "                d[g] = d.get(g, 0.0) + float(v)\n",
    "        if n_samples == 1:\n",
    "            s = pd.Series(per_sample[0], dtype=float)\n",
    "            if s.sum() > 0:\n",
    "                s = s / s.sum()\n",
    "            return s.sort_values(ascending=False)\n",
    "        else:\n",
    "            dfs = []\n",
    "            for vec in per_sample:\n",
    "                s = pd.Series(vec, dtype=float)\n",
    "                s = s / s.sum() if s.sum() > 0 else s\n",
    "                dfs.append(s)\n",
    "            avg = pd.concat(dfs, axis=1).fillna(0.0).mean(axis=1)\n",
    "            return avg.sort_values(ascending=False)\n",
    "\n",
    "def read_tsv_genus(path: Path) -> pd.Series:\n",
    "    raise NotImplementedError(\"Включи ALLOW_TSV и напиши разбор TSV, если он понадобится.\")\n",
    "\n",
    "# --- сборка ---\n",
    "records, labels, labels_onto, logs = {}, {}, {}, []\n",
    "files = list(DATA_DIR.rglob(\"*.biom\"))\n",
    "if ALLOW_TSV:\n",
    "    files += list(DATA_DIR.rglob(\"*.tsv\"))\n",
    "\n",
    "for p in sorted(files):\n",
    "    name = p.name\n",
    "    label, tag = parse_label_from_new(name)\n",
    "    if label is None:\n",
    "        label, tag = parse_label_from_old(name)\n",
    "    if label is None:\n",
    "        logs.append(f\"SKIP (no canonical label): {name}\")\n",
    "        continue\n",
    "    try:\n",
    "        if p.suffix == \".biom\":\n",
    "            vec = read_biom_hdf5_genus(p)\n",
    "        else:\n",
    "            vec = read_tsv_genus(p)\n",
    "        vec = vec.head(2000)  # мягкий срез хвоста\n",
    "        base = clean_stem(p.stem)\n",
    "        sid = f\"{base}\"\n",
    "        i, key = 1, sid\n",
    "        while key in records:\n",
    "            i += 1\n",
    "            key = f\"{sid}__{i}\"\n",
    "        records[key] = vec\n",
    "        labels[key]  = label\n",
    "        labels_onto[key] = CLASS_CANON[label]\n",
    "        logs.append(f\"OK: {name} → {key} → {len(vec)} genera, label={label}\")\n",
    "    except Exception as e:\n",
    "        logs.append(f\"FAIL: {name} → {e}\")\n",
    "\n",
    "X = pd.DataFrame(records).T.fillna(0.0)\n",
    "y = pd.Series(labels, name=\"label\")\n",
    "y_onto = pd.Series(labels_onto, name=\"label_ontology\")\n",
    "\n",
    "print(\"Shape:\", X.shape)\n",
    "print(\"Class counts:\\n\", y.value_counts())\n",
    "print(\"\\n\".join(logs[:10]), \"\\n...\", f\"({len(logs)} lines)\")\n",
    "\n",
    "X.to_csv(\"X_genus_matrix.csv\")\n",
    "y.to_csv(\"y_labels.csv\")\n",
    "y_onto.to_csv(\"y_labels_ontology.csv\")\n",
    "with open(\"build_log.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(logs))\n",
    "\n",
    "print(\"\\nSaved: X_genus_matrix.csv, y_labels.csv, y_labels_ontology.csv, build_log.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5cac37",
   "metadata": {},
   "source": [
    "2) Финальные пайплайны (с CLR и фильтрацией)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b019d63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# CLR, совместимый с NumPy (работает и на DataFrame, и на ndarray)\n",
    "def clr_np(X, pseudo=1e-6):\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    Xp = X + pseudo\n",
    "    logX = np.log(Xp)\n",
    "    gm = logX.mean(axis=1, keepdims=True)  # геом. среднее в лог-пространстве (по строкам)\n",
    "    return logX - gm\n",
    "\n",
    "var_filter = VarianceThreshold(threshold=1e-6)\n",
    "\n",
    "pipe_lr = Pipeline([\n",
    "    (\"var\", var_filter),\n",
    "    (\"clr\", FunctionTransformer(clr_np, validate=False)),\n",
    "    (\"sc\",  StandardScaler(with_mean=False)),\n",
    "    (\"lr\",  LogisticRegression(\n",
    "        penalty=\"elasticnet\", solver=\"saga\",\n",
    "        l1_ratio=0.5, C=1.0,\n",
    "        class_weight=\"balanced\",\n",
    "        max_iter=6000, n_jobs=-1, random_state=42\n",
    "    )),\n",
    "])\n",
    "\n",
    "pipe_rf = Pipeline([\n",
    "    (\"var\", var_filter),\n",
    "    (\"rf\", RandomForestClassifier(\n",
    "        n_estimators=900, max_depth=None, max_features=\"sqrt\",\n",
    "        min_samples_leaf=1, class_weight=\"balanced\",\n",
    "        random_state=42, n_jobs=-1\n",
    "    )),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2694e21",
   "metadata": {},
   "source": [
    "3) Честное сравнение на GroupKFold (основная метрика)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bdf92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Загрузка X/y, если не в памяти (на случай перезапуска)\n",
    "if 'X' not in globals() or 'y' not in globals():\n",
    "    X = pd.read_csv(\"X_genus_matrix.csv\", index_col=0)\n",
    "    y = pd.read_csv(\"y_labels.csv\", index_col=0)[\"label\"]\n",
    "\n",
    "# CLR, совместимый с NumPy\n",
    "def clr_np(X, pseudo=1e-6):\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    Xp = X + pseudo\n",
    "    logX = np.log(Xp)\n",
    "    gm = logX.mean(axis=1, keepdims=True)\n",
    "    return logX - gm\n",
    "\n",
    "var_filter = VarianceThreshold(threshold=1e-6)\n",
    "\n",
    "pipe_lr = Pipeline([\n",
    "    (\"var\", var_filter),\n",
    "    (\"clr\", FunctionTransformer(clr_np, validate=False)),\n",
    "    (\"sc\",  StandardScaler(with_mean=False)),\n",
    "    (\"lr\",  LogisticRegression(\n",
    "        penalty=\"elasticnet\", solver=\"saga\",\n",
    "        l1_ratio=0.5, C=1.0,\n",
    "        class_weight=\"balanced\",\n",
    "        max_iter=6000, n_jobs=-1, random_state=42\n",
    "    )),\n",
    "])\n",
    "\n",
    "pipe_rf = Pipeline([\n",
    "    (\"var\", var_filter),\n",
    "    (\"rf\", RandomForestClassifier(\n",
    "        n_estimators=900, max_depth=None, max_features=\"sqrt\",\n",
    "        min_samples_leaf=1, class_weight=\"balanced\",\n",
    "        random_state=42, n_jobs=-1\n",
    "    )),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364c0c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR+CLR: (np.float64(0.96), np.float64(0.07999999999999999)) RF: (np.float64(0.96), np.float64(0.07999999999999999))\n",
      "Chosen: LR+CLR\n"
     ]
    }
   ],
   "source": [
    "import re, numpy as np, pandas as pd\n",
    "from sklearn.model_selection import GroupKFold, cross_val_score\n",
    "\n",
    "# если в индексе нет MGYS/MGYA/ERR/SRR, группа = сам sample_id (уникально)\n",
    "def extract_group(s):\n",
    "    m = re.search(r\"(MGYS\\d+|MGYA\\d+|ERR\\d+|SRR\\d+)\", s)\n",
    "    return m.group(1) if m else s\n",
    "\n",
    "groups = [extract_group(i) for i in X.index]\n",
    "cv = GroupKFold(n_splits=min(5, len(set(groups))))\n",
    "\n",
    "\n",
    "def gkf_score(model):\n",
    "    s = cross_val_score(model, X, y, cv=cv.split(X, y, groups), scoring=\"balanced_accuracy\", n_jobs=-1)\n",
    "    return s.mean(), s.std()\n",
    "\n",
    "s_lr = gkf_score(pipe_lr)\n",
    "s_rf = gkf_score(pipe_rf)\n",
    "print(\"LR+CLR:\", s_lr, \"RF:\", s_rf)\n",
    "\n",
    "best_model = pipe_lr if s_lr[0] >= s_rf[0] else pipe_rf\n",
    "best_name  = \"LR+CLR\" if best_model is pipe_lr else \"RF\"\n",
    "print(\"Chosen:\", best_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3170d0",
   "metadata": {},
   "source": [
    "4) Обучить лучший на всём X и сохранить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2dd8e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: soil_biome_classifier.joblib\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "best_model.fit(X, y)\n",
    "joblib.dump({\n",
    "    \"pipeline\": best_model,\n",
    "    \"feature_names\": X.columns.tolist(),\n",
    "    \"classes\": sorted(y.unique().tolist())\n",
    "}, \"soil_biome_classifier.joblib\")\n",
    "\n",
    "print(\"Saved: soil_biome_classifier.joblib\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
